{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/share/virtualenvs/Comparing-Measures-of-LLM-Truthfulness-0AmlYu00/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('LieDetection/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SimpleTQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpleTQA import SIMPLE_TQA_QUESTION_PREFIX, SIMPLE_TQA_ANSWER_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqa = datasets.load_dataset(\"truthful_qa\", \"multiple_choice\")\n",
    "tqa = pd.DataFrame(tqa[\"validation\"])\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "full_prompts = []\n",
    "\n",
    "for _, row in tqa.iterrows():\n",
    "    choices = row[\"mc1_targets\"][\"choices\"]\n",
    "    correct_choice_idx = row[\"mc1_targets\"][\"labels\"].index(1)\n",
    "    false_choice_indices = list(range(len(choices))) \n",
    "    false_choice_indices.remove(correct_choice_idx)\n",
    "    false_choice_idx = np.random.choice(false_choice_indices)\n",
    "    \n",
    "    question_with_true_answer = f\"Question: {row['question']}\\nAnswer: {choices[correct_choice_idx]}\"\n",
    "    full_prompt_true_answer = f\"{SIMPLE_TQA_QUESTION_PREFIX}{question_with_true_answer}\\n{SIMPLE_TQA_ANSWER_PREFIX}\" \n",
    "    questions.append(question_with_true_answer)\n",
    "    answers.append(\"True\")\n",
    "    full_prompts.append(full_prompt_true_answer)\n",
    "    \n",
    "    question_with_false_answer = f\"Question: {row['question']}\\nAnswer: {choices[false_choice_idx]}\"\n",
    "    full_prompt_false_answer = f\"{SIMPLE_TQA_QUESTION_PREFIX}{question_with_false_answer}\\n{SIMPLE_TQA_ANSWER_PREFIX}\" \n",
    "    questions.append(question_with_false_answer)\n",
    "    answers.append(\"False\")\n",
    "    full_prompts.append(full_prompt_false_answer)\n",
    "\n",
    "simple_tqa = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"full_prompt\": full_prompts,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tqa.to_csv(\"data/simple_tqa.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SimpleFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "facts = pd.read_csv(\"RepresentationEngineering/data/facts/facts_true_false.csv\")\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for _, row in facts.iterrows():\n",
    "    statement = row[\"statement\"]\n",
    "    label = row[\"label\"]\n",
    "    prepared_statement = statement[0].lower() + statement[1:-1] + \"?\"\n",
    "    question = \"Is it true that \" + prepared_statement\n",
    "    questions.append(question)\n",
    "    answers.append(\"Yes\" if label else \"No\")\n",
    "    \n",
    "simple_facts = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_facts.to_csv(\"data/simple_facts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it true that the Earth's atmosphere protects us from harmful radiation from the sun?\n"
     ]
    }
   ],
   "source": [
    "print(simple_facts[\"question\"].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Comparing-Measures-of-LLM-Truthfulness-c5kw3004",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
