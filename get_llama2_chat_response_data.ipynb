{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from llama2_utils import establish_llama2_endpoint\n",
    "from simpleTQA import SimpleTQA \n",
    "from simpleFacts import SimpleFacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama-2-7b-chat\"\n",
    "llama2_endpoint = establish_llama2_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_questions_to_answer = ...\n",
    "\n",
    "def find_number_of_rows_with_questions_the_model_can_answer(number_answerable_questions_required, dataset, model):\n",
    "    return np.where(dataset[f\"{model}_can_answer\"].cumsum() == number_answerable_questions_required)[0][0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_data(dataset, model):\n",
    "\n",
    "    dataset.check_if_model_can_answer(\n",
    "        model=model,\n",
    "        model_kwargs={\n",
    "            \"endpoint\": llama2_endpoint,\n",
    "            \"max_tokens\": 64,\n",
    "            \"stop\": \"\\n\"\n",
    "        },\n",
    "        max_batch_size=20,\n",
    "        save_progress=True,\n",
    "        bypass_cost_check=True,\n",
    "    )\n",
    "\n",
    "    print(f\"CAN_{model.upper()}_ANSWER COMPLETED CORRECTLY\")\n",
    "\n",
    "    answered_correctly = dataset[f\"{model}_can_answer\"].sum()\n",
    "    attempted = dataset[f\"{model}_can_answer\"].count()\n",
    "    print(\"Answered correctly: \", answered_correctly)\n",
    "    print(\"Attempted: \", attempted)\n",
    "\n",
    "\n",
    "    try:\n",
    "        n_rows = find_number_of_rows_with_questions_the_model_can_answer(number_questions_to_answer, dataset, model)\n",
    "    except IndexError:\n",
    "        # the above gives index_error if there are less questions that the model can answer in the dataset than\n",
    "        # number_questions_to_answer; in that case, fix n_rows to len(dataset)\n",
    "        n_rows = len(dataset)\n",
    "\n",
    "    dataset.does_model_lie(\n",
    "        max_questions_to_try=n_rows,\n",
    "        model=model,\n",
    "        model_kwargs={\"endpoint\": llama2_endpoint, \"max_tokens\": 64, \"stop\": \"\\n\"},\n",
    "        # max_questions_to_try=10,\n",
    "        max_batch_size=20,\n",
    "        save_progress=True,\n",
    "        bypass_cost_check=True,\n",
    "    )\n",
    "\n",
    "    print(f\"DOES_{model.upper()}_LIE COMPLETED CORRECTLY\")\n",
    "\n",
    "    lying_rate = dataset.compute_lying_ratio()\n",
    "    double_down_rate = dataset.compute_lie_double_down_rate()\n",
    "\n",
    "    print(\"lying_rate\", lying_rate)\n",
    "    print(\"double_down_rate\", double_down_rate)\n",
    "\n",
    "    dataset.generate_logprobs(\n",
    "        max_questions_to_try=n_rows,\n",
    "        model_suspect=model,\n",
    "        model_kwargs_suspect={\"endpoint\": llama2_endpoint, \"max_tokens\": 64, \"stop\": \"\\n\"},\n",
    "        # max_questions_to_try=10,\n",
    "        save_progress=True,\n",
    "    )\n",
    "\n",
    "    print(f\"GENERATE_{model.upper()}_LOGPROBS COMPLETED CORRECTLY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tqa = SimpleTQA()\n",
    "get_response_data(simple_tqa, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_facts = SimpleFacts()\n",
    "get_response_data(simple_facts, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
